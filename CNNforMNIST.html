<!DOCTYPE HTML>
<html>
	<head>
		<title>CNN for MNIST — aaronguan.</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
        <link rel="shortcut icon" href="icon.jpg">
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
            <h1><a href="index.html">HOME</a></h1>
            <nav class="links">
                    <ul>
                        <li><a href="about.html">ABOUT ME</a></li>
                        <li><a href="things-i-saw.html">THINGS I SAW</a></li>
                        <li><a href="things-i've-made.html">THINGS I HAVE MADE</a></li>
                        <li><a href="things-i-wanna-say.html">THINGS I WANNA SAY</a></li>
                        <li><a href="other-thing.html">OTHER THINGS</a></li>
                    </ul>
            </nav>
            <nav class="main">
                <ul>
                    <li class="menu">
                        <a class="fa-bars" href="#menu">Menu</a>
                    </li>
                </ul>
            </nav>
        </header>
        <!-- Menu -->
        <section id="menu">
            <!-- Links -->
                <section>
                    <ul class="links">
                        <li>
                            <a href="about.html">
                                <h3>ABOUT ME</h3>
                                <p>Come to know more about me</p>
                            </a>
                        </li>
                        <li>
                            <a href="things-i-saw.html">
                                <h3>THINGS I SAW</h3>
                                <p>Lovely things I captured in the world</p>
                            </a>
                        </li>
                        <li>
                            <a href="things-i've-made.html">
                                <h3>THINGS I HAVE MADE</h3>
                                <p>Some interesting things I created</p>
                            </a>
                        </li>
                        <li>
                            <a href="things-i-wanna-say.html">
                                <h3>THINGS I WANNA SAY</h3>
                                <p>I'd like to say something</p>
                            </a>
                        </li>
                        <li>
                            <a href="other-thing.html">
                                <h3>OTHER THINGS</h3>
                                <p>Some little things I wanna write down</p>
                            </a>
                        </li>
                    </ul>
                </section>

					</section>
                    <div class="copyrights">
						&copy; Aaron Guan. All rights reserved. Design: <a href="http://www.aaronguan.com/">Aaron Guan</a>.
					</div>
				<!-- Main -->
					<div id="main">
                        <div id="postwrapper">
						<!-- Post -->
							<article class="post">
								<header>
									<div class="title">
										<h2>Convolutional Neural Networks (CNN) for MNIST Dataset</h2>
										<p>Feb 03, 2018</p>
									</div>
								</header>
                                <p>I implemented a basic Convolutional Neural Networks (CNN) to recognize the handwritten digits in the MNIST data set.</p>
                            <p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> comprises 60,000 training examples and 10,000 test examples of the handwritten digits 0–9, formatted as 28x28-pixel monochrome images.</p>
                            <h3>1. Define the Functions</h3>
                            Basically, we need to implement a function: y = softmax(Wx+b), where W is the filter, x is the input image, b is the bias, and y is the output "label".
                            <p><b>1.1 &emsp;The kernel filter</b></p>
                            <pre><code>def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
</code></pre>
                            <p>shape = [height, width, in_channels, out_channels]<br><code>tf.truncated_normal()</code> keeps random numbers in the range of [mean - 2 × stddev, mean + 2 × stddev]</p>
                            <p><b>1.2 &emsp;The bias</b></p>
                            <pre><code>def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)</code></pre>
                            <p>A tensor of the specified shape filled with constant 0.1</p>
                            <p><b>1.3 &emsp;The convolution</b></p>
                            <pre><code>def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')</code></pre>
                            <code>tf.nn.conv2d(input, filter, strides, padding)</code>
                            <p>[Input image]: shape = [batch_size, in_height, in_width, in_channels]<br>
                            [Filter]: shape = [filter_height, filter_width, input channel, output channels]<br>
                            [Stride]: shape = [batch_stride, height_stride, width_stride, channels_stride]<br>
                            [Padding]: 'SAME' or 'VALID'<br>
                            &emsp;SAME: new_height = imageHeight_initial / strideHeight, same for width. Add the padding to the input image, so after filtered, the image is the same as initial image.<br>
                            &emsp;VALID: new_height = imageHeight_initial - filterHeight + strideHeight, same for width</p>
                            <p><b>1.4 &emsp;The pooling</b></p>
                            <pre><code>def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')</code></pre>
                            <code>tf.nn.max_pool(input, ksize, strides, padding)</code>
                            <p>[Input image]: shape = [batch_size, in_height, in_width, in_channels]<br>
                            [ksize]: size of the pooling window.<br>
                            [Stride] &amp; [Padding]: Same as above.</p>
                            <h3>2. Create the CNN structure</h3>
                            <p><b>2.1 &emsp;Input Image</b></p>
                            <pre><code>x = tf.placeholder(tf.float32, [None, 784])
x_image = tf.reshape(x, [-1, 28, 28, 1])</code></pre>
                            <p><code>tf.placeholder()</code> The value that we will input when we run the program.<br>
                            We wanna input any number (batch-size) of MNIST images, each flattened into a <b>784</b>-dimensional vector. Why 784? Because the dimensionality of a single flattened MNIST image is: 28*28 = 784. We represent this as a 2-D tensor of floating-point numbers. <b>None</b> indicates that the first dimension, corresponding to the batch size, can be of any size.</p>
                            <p><b>2.2 &emsp;The First Convolutional Layer</b></p>
                            <pre><code># Filters: 32 kernels with 5x5 size
W_conv1 = weight_variable([5, 5, 1, 32]) # Filter shape = [height, width, in_channels, out_channels]
# biase
b_conv1 = bias_variable([32]) # 32 kernels -> 32 outputs -> 32 bias
# Convolution
h_conv1 = tf.nn.elu(conv2d(x_image, W_conv1) + b_conv1) # activation function: ReLu
# pooling
h_pool1 = max_pool_2x2(h_conv1)</code></pre>
                            <p><b>2.3 &emsp;The Second Convolutional Layer</b></p>
                            <pre><code># Filters: 32 input from last layer and 64 output -> 64 kernels of 5*5
W_conv2 = weight_variable([5, 5, 32, 64])
# bias
b_conv2 = bias_variable([64])
# Convolution
h_conv2 = tf.nn.elu(conv2d(h_pool1, W_conv2) + b_conv2) # activation function : ReLu
# pooling
h_pool2 = max_pool_2x2(h_conv2)</code></pre>
                            <p><b>2.4 &emsp;The Fully Connected Layer</b></p>
                            <pre><code># Filters: input is 7*7*64, output is 1024
W_fc1 = weight_variable([7 * 7 * 64, 1024])
# bias
b_fc1 = bias_variable([1024])
# flatten the input image into 1-D vector of 7*7*64
h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])
h_fc1 = tf.nn.elu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) # tf.matmul: one-one multiply for two vector
# Dropout
keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</code></pre>
                            <p>To reduce <b>over-fitting</b>, we will apply <b>dropout</b> before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. </p>
                            <p><b>2.5 &emsp;The Output Layer, also a fully connected layer</b></p>
                            <pre><code># Filters: input is 1024, output is 10 (10 labels/classes for output)
W_fc2 = weight_variable([1024, 10])
# bias
b_fc2 = bias_variable([10])
# The actual output
y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)# activation function: Softmax
# The expect output
y_ = tf.placeholder(tf.float32, [None, 10])</code></pre>
                            <p>The output <b>y_conv</b> and expect output <b>y_</b> consist of a 2d tensor, where each row is a one-hot 10-dimensional vector indicating which digit class (0~9) the MNIST image belongs to.</p>
                            <h3>3. loss and accuracy</h3>
                            <p>For both training and evaluation, we need to define a loss function that measures how closely the model's predictions match the target classes. We try to minimize the loss while training across all the examples. For multiclass classification problems like MNIST, <b>cross entropy</b> is typically used as the loss metric. The definition of the cross entropy is:</p>
                            <img src="https://latex.codecogs.com/gif.latex?H_{yâ}\left(&space;y\right)=-\sum&space;_{i}y_{i}â\log&space;\left(&space;y_{i}\right)" title="H_{yâ}\left( y\right)=-\sum _{i}y_{i}â\log \left( y_{i}\right)" />
                            <pre><code>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) #Adam Optimize</code></pre>
                            <p>Calculate the <b>accuracy</b> to see whether the actual output y_conv is equal to the target output y_. <code>tf.argmax(y_conv,1)</code> is the label our model thinks is most likely for each input, while <code>tf.argmax(y_,1)</code> is the true label. We can use tf.equal to check if our prediction matches the truth.</p>
                            <pre><code>correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</code></pre>
                            <p><code>tf.argmax</code>: Return the index of the highest entry in a tensor along some axis.<br>
                            <code>tf.equal</code>: Return true if equal, otherwise, return false.<br>
                            <code>tf.cast</code>: Cast the boolean type array into int type array.<br>
                            <code>tf.reduce_mean</code>: Calculate the mean to represent the accuracy.</p>
                            <h3>4. Training and Testing</h3>
                            <p><b>4.1 &emsp;Load Data and Start TensorFlow InteractiveSession</b></p>
                            <pre><code>from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_string('data_dir', '/tmp/data/', 'Directory for storing data')
print("Data Directory:" + FLAGS.data_dir)
mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
# Start the Session
session = tf.InteractiveSession()
session.run(tf.global_variables_initializer())</code></pre>
                            <p>The images are returned as a 2-D NumPy array of size <b>[batch_size, 784]</b> (Because 784 pixels in an MNIST image), and the labels are returned as 2-D NumPy array of size <b>[batch_size, 10]</b></p>
                            <p><b>4.2 &emsp;Train and Evaluate the Model</b></p>
                            <pre><code>for i in range(3000): # Train 3000 times
    batch = mnist.train.next_batch(50) # Pick 50 datasets for everytime train
    # mnist.train.next_batch(batch_size)
    # Returns a tuple of 2 arrays, where the first represents a batch of batch_size MNIST images,
    # and the second represents a batch of batch-size labels corresponding to those images.
    if i % 100 == 0:
        # eval means run
        # Using feed_dict to replace the placeholder tensors x, y_, and keep_prob.
        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})
        print("The accuracy of the %d training cycle: %g" % (i, train_accuracy))

    # when train_step run, it will apply ADAM optimizer to the parameters
    # Training the model can therefore be accomplished by repeatedly running train_step.
    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print("The accuracy of the testing: %g" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))</code></pre>
                            <h3>5. Results</h3>
                            <p>After training 3000 times, the model can achieve 97.69% accuracy.</p>
                            <p>The codes can be found at the <a href="https://github.com/aaronzguan/CNNforMNIST">GitHub</a>. You can also <a href="mailto:aaron.z.guan@hotmail.com">email</a> me if you have any findings or questions.</p>
								<footer class="actions">
									<a href="things-i've-made.html" class="button big">BACK</a>
								</footer>
                        </article>
					</div>
                </div>
			</div>
        <!-- Footer -->
        <div id="footer" align="center">
            <ul class="icons"> 
                <li><a href="https://www.facebook.com/profile.php?id=100004606699026" class="fa-facebook"><span class="label">Facebook</span></a></li>
                <li><a href="https://www.instagram.com/mraaronever/" class="fa-instagram"><span class="label">Instagram</span></a></li>
                <li><a href="https://www.youtube.com/channel/UCbl3QhmOwDUxx3JBIaQ8QyA" class="fa-youtube"><span class="label">Youtube</span></a></li>
                <li><a href="mailto:aaron.z.guan@hotmail.com" class="fa-envelope"><span class="label">Email</span></a></li>
                <li><a href="https://github.com/aaronzguan" class="fa-github"><span class="label">GitHub</span></a></li>
                <li><a href="https://www.linkedin.com/in/zhongguan/" class="fa-linkedin"><span class="label">LinkedIn</span></a>
            </ul>
            <p class="copyright">Copyright &copy; <a href="index.html">Aaron Z Guan</a> 2017.</p>
        </div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>